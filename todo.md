# Что в этой ветке

Планируется общий рефакторинг и исправление ошибок, без глобальных фичей и т.д.

Что будет:

- [x] Ошибка когда узел не коннектится к другому узлу - просто встает и все

Решения по факту нет - это проблема уже операционной системы.
Я не могу отследить когда запрос попадает в очередь запросов внутри ОС и следить за ходом его обработки,
поэтому задержки между отправкой запроса присоединения и фактическим принятием запроса -БУДУТ ВСЕГДА.

Но в рамках этой задачи был исправлен критический баг, из-за которого неправильно отслеживался терм у обработчика узлов
в
состоянии лидера - текущим термом считался ТЕКУЩИЙ терм узла, а не терм в котором вошли в состояние лидера.
Из-за этого возникали трудно воспроизводимые баги, когда узел падал без понятных на то причин.

- [x] Отслеживание лидера

Крч, починил тем, что IRaftConsensusModule теперь сам отслеживает лидера - не сторонний декоратор.
Обновляется при каждом успешном запросе к последователю (остальные состояния возвращают либо свой ID (лидер), либо
null (кандидат)).

Появилась другая проблема - ClusterOptions теперь зависит от этого модуля для своевременного получения лидера кластера и
когда реализовал, возникла циклическая зависимость. Решил тем, что убрал ClusterInfo из обязательных свойств
IApplication - пока он никому
не нужен, так что не смертельно.

- [x] Stream для ISnapshot

Поток для снапшота реализован, но его единственное применение - только во время восстановления состояния.
При записи снапшота необходим более четкий контроль над записью в файл.

- [ ] Таймаут ожидания при ошибке подключения к другому узлу (чтобы не спамил запросами)
- [x] Рефакторинг архитектуры по поводу работы с другими узлами

Завожу массив из потоков обработчиков. Каждый такой поток представляет собой примерно следующее:

```csharp
void Run()
{
    // 1
    foreach (var (worker, token) in requests)
    {
        // 2
        using var cts = CancellationTokenSource.CreateLinkedSource(token);
        try
        {
            // 3
            worker.Run(cts.Token);
        catch (OperationCancelledException)
        { }
    }
}
```

Сама функция - это главная функция всего потока. На каждый узел создается один такой поток и дальше используется.
Пояснения:

1. Я читаю запросы из какого-то места.
   Здесь это `requests`, но по факту каждый вызывающий может добавить свой запрос в эту очередь запросов  
   и я поочередно буду брать воркеров и запускать их
2. У меня есть 2 источника отмены: передающийся токен и мой токен. Передающийся нужен, чтобы остановить воркера, а мой -
   чтобы знать когда нужно закончить работу
3. Каждый воркер будет представляться каким нибудь интерфейсом с подобным методом (`Run`)

Воркеры могут быть 2 типов:

- Запуск кворума (Кандидат)
- Обработка других узлов (Лидер)

Если мы последователь, то тут потоки просто будут заблокированы до лучших времен, т.е. процессорное время занимать не
будут.

Можно использовать уже готовый `BlockingChannel` для этого. Он как раз реализован.

Шаги реализации:

1. Добавить новый интерфейс IWorker или типа того
2. Создать новый объект потока обработчика узла + все нужные методы
3. Реализовать IBackgroundJobQueue через подобный поток
4. Найти способ указывать какой именно поток использовать
5. Заменить асинхронность на синхронность везде

> Замечание: какой поток использовать можно понять:
> 1. добавить в интерфейс поле NodeId;
> 2. в интерфейсе IPeer уже есть нужное свойство.

- [x] Заменить BinaryPacketSerializer на прямую сериализацию самих пакетов

Во-первых, переименовал префикс имени межузлового пакета из `Raft` в `Node`. Т.е. теперь
это `NodePacket`, `NodePacketType` и т.д.

Во-вторых, вынес логику сериализации/десериализации в сам класс `NodePacket` (`BinaryPacketSerializer` удален).

В-третьих, все классы (файлы) из старого проекта (csproj) перенес в `Network` либо удалил (место освободил).

- [x] Улучшенная обработка ошибок маппинга запросов на команды

Доработал логику создания команды создания очереди - теперь он принимает `QueueImplementationDetails`, который сам в
себе уже хранит нужные данные для создания очереди -
там проверяются базовые параметры (размер очереди, необходимые поля и т.д.) + для каждой ошибки - свое исключение.

Добавил больше кодов ошибок бизнес-логики (большая часть добавленных - из-за создания очереди).

Теперь при маппинге такие исключения отлавливаются и бросается более общее исключение `MappingException`, в котором есть
код ошибки, который нужно вернуть клиенту.

- [x] Рефакторинг стартапа (как ASP.NET Core) - регистрация сервисов, настройка конфигурации

Что тут нужно будет сделать:

1. Добавить какой-нибудь DI контейнер (найти)
2. Общий класс для конфигурации
3. Валидации конфигурации при старте

При старте:

- Настроить `ThreadPool.SetMaxThreads`
- Настройка логера
- Регистрация обработчика `CancelKeyPress`
- Для отслеживания завершения реботы - `CancellationTokenSource`
- Таймаут завершения работы приложения
- Чтение файлов нужных + проверка валидности (чек-суммы и т.д.)

Откуда брать конфигурацию:

- Переменные окружения
- Аргументы командной строки
- Файл с конфигурацией

- [x] Уменьшить общее кол-во проектов

- [x] Логирование настроить

- [ ] Рефакторинг работы с диском (незакоммиченные записи на диске хранить и динамически этот буфер) + вынести отдельный
  интерфейс для хранилища
- [ ] Остановку работы нормально реализовать (сейчас токен не отменяется нормально + завершение работы фатальное при
  исключении (общий Application класс))

На след итерацию:

- PreRequestVote
- Поддержка кластера из 1 узла (сразу лидером становлюсь)
- Prometheus метрики (+ нужно свой какой-нибудь HTTP сервер запустить, м.б. Kestrel стандартный)